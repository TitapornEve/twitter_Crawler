{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "twitterCrawler.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMQX3VoN2OD7Z6D0lRcVWXC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TitapornEve/twitter_Crawler/blob/main/twitterCrawler.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I6oSQv0kvFKi",
        "outputId": "2c73c3d8-8fe7-4e17-96ab-878b19bcb0d9"
      },
      "source": [
        "!pip install pythainlp\r\n",
        "!pip install emoji"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pythainlp\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c1/09/1215cb6f6ef0cfc9dbb427a961fda8a47c111955f782f659ca2d38c79adc/pythainlp-2.2.6-py3-none-any.whl (10.6MB)\n",
            "\u001b[K     |████████████████████████████████| 10.6MB 7.0MB/s \n",
            "\u001b[?25hCollecting python-crfsuite>=0.9.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/95/99/869dde6dbf3e0d07a013c8eebfb0a3d30776334e0097f8432b631a9a3a19/python_crfsuite-0.9.7-cp36-cp36m-manylinux1_x86_64.whl (743kB)\n",
            "\u001b[K     |████████████████████████████████| 747kB 33.7MB/s \n",
            "\u001b[?25hCollecting tinydb>=3.0\n",
            "  Downloading https://files.pythonhosted.org/packages/af/cd/1ce3d93818cdeda0446b8033d21e5f32daeb3a866bbafd878a9a62058a9c/tinydb-4.4.0-py3-none-any.whl\n",
            "Requirement already satisfied: requests>=2.22.0 in /usr/local/lib/python3.6/dist-packages (from pythainlp) (2.23.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.22.0->pythainlp) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.22.0->pythainlp) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.22.0->pythainlp) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.22.0->pythainlp) (2020.12.5)\n",
            "Installing collected packages: python-crfsuite, tinydb, pythainlp\n",
            "Successfully installed pythainlp-2.2.6 python-crfsuite-0.9.7 tinydb-4.4.0\n",
            "Collecting emoji\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/24/fa/b3368f41b95a286f8d300e323449ab4e86b85334c2e0b477e94422b8ed0f/emoji-1.2.0-py3-none-any.whl (131kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 5.2MB/s \n",
            "\u001b[?25hInstalling collected packages: emoji\n",
            "Successfully installed emoji-1.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJKgYjadvQAJ"
      },
      "source": [
        "import tweepy\r\n",
        "import pandas as pd\r\n",
        "from sklearn.feature_extraction.text import CountVectorizer\r\n",
        "import numpy as np\r\n",
        "import emoji\r\n",
        "from pythainlp.tokenize import word_tokenize\r\n",
        "from pythainlp.corpus import thai_stopwords\r\n",
        "import re\r\n",
        "from wordcloud import WordCloud\r\n",
        "import matplotlib.pyplot as plt "
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dyW_YUTAvT8n"
      },
      "source": [
        "consumer_key =  '2AVhE9ns3kfaiZAaCSuClF6bw'\r\n",
        "consumer_secret = 'IGN07yckRme03ZoZn788gkg8Yjt6PvKgu7BzSPsxWyPHkxxxdA'\r\n",
        "access_token = '1362120123339837440-YhXgRzhwnSdnF4E1veeQaxQIyEjbcp'\r\n",
        "access_token_secret = 'CRQhrKTHAAKOnBCVae6p9cjcUBjcaJiZMQz4YCFPB3pu3'\r\n",
        "\r\n",
        "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\r\n",
        "auth.set_access_token(access_token, access_token_secret)\r\n",
        " \r\n",
        "api = tweepy.API(auth)\r\n",
        "\r\n",
        "query = \"#เด็กคลองหก\" \r\n",
        "\r\n",
        "df = pd.DataFrame(columns=[\"create_at\",\"text\",\"hashtag\",\"retweet_count\",\"favourite_count\"])  \r\n",
        "for tweet in tweepy.Cursor(api.search,q=query,count=100,result_type=\"recent\",tweet_mode='extended').items():\r\n",
        "    entity_hashtag = tweet.entities.get('hashtags')\r\n",
        "    hashtag = \"\"\r\n",
        "    for i in range(0,len(entity_hashtag)):\r\n",
        "        hashtag = hashtag +\"/\"+entity_hashtag[i][\"text\"]\r\n",
        "    re_count = tweet.retweet_count\r\n",
        "    create_at = tweet.created_at\r\n",
        "    try:\r\n",
        "        text = tweet.retweeted_status.full_text\r\n",
        "        fav_count = tweet.retweeted_status.favorite_count\r\n",
        "    except:\r\n",
        "        text = tweet.full_text\r\n",
        "        fav_count = tweet.favorite_count\r\n",
        "    new_column = pd.Series([create_at,text,hashtag,re_count,fav_count], index=df.columns)\r\n",
        "    df = df.append(new_column,ignore_index=True)\r\n",
        "\r\n",
        "df.to_excel(\"twitterCrawler7.xlsx\")"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lamjObIevZCI"
      },
      "source": [
        "df = pd.read_excel(\"twitterCrawler7.xlsx\")"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NuA37V5qveoG"
      },
      "source": [
        "df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cALiFKr5vhes"
      },
      "source": [
        "df.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zF1qLcw7voME"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bX7f1sg8vq9e"
      },
      "source": [
        "df[\"date\"] = df['create_at'].dt.date"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2kr6afLcv6AD"
      },
      "source": [
        "df.groupby('date').sum()[['retweet_count']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lZ13xLwIv78a"
      },
      "source": [
        "df.drop_duplicates(\"text\").sort_values(by=['retweet_count'], ascending=False).head(10)[['text','retweet_count']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MNQWFgoMv_sg"
      },
      "source": [
        "df.drop_duplicates(\"text\").sort_values(by=['favourite_count'], ascending=False).head(10)[['text','favourite_count']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78nZLrafwI-3"
      },
      "source": [
        "def slash_tokenize(d):  \r\n",
        "    result = d.split(\"/\")\r\n",
        "    result = list(filter(None, result))\r\n",
        "    return result\r\n",
        "\r\n",
        "hastag_data = df[\"hashtag\"].dropna()\r\n",
        "vectorizer = CountVectorizer(tokenizer=slash_tokenize)\r\n",
        "transformed_data = vectorizer.fit_transform(hastag_data)\r\n",
        "hash_tag_cnt_df= pd.DataFrame(columns = ['word', 'count']) \r\n",
        "hash_tag_cnt_df['word'] = vectorizer.get_feature_names()\r\n",
        "hash_tag_cnt_df['count'] = np.ravel(transformed_data.sum(axis=0))\r\n",
        "hash_tag_cnt_df.sort_values(by=['count'], ascending=False).head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6u2fvjMvwOef"
      },
      "source": [
        "def cleanText(text):\r\n",
        "    text = str(text)\r\n",
        "    text = text.replace('\\n','')\r\n",
        "    allchars = [str for str in text]\r\n",
        "    emoji_list = [c for c in allchars if c in emoji.UNICODE_EMOJI]\r\n",
        "    text = ' '.join([str for str in text.split() if not any(i in str for i in emoji_list)])\r\n",
        "    text = text.replace('\"',\"\")\r\n",
        "    text = text.replace(\"!\",\"\")\r\n",
        "    text = text.replace(\"​\",\"\")\r\n",
        "    text = text.replace(\"-\",\"\")\r\n",
        "    text_split = text.split(\" \")\r\n",
        "    text = \" \".join([c for c in text_split if \"#\" not in c])\r\n",
        "    text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)   \r\n",
        "    stop_word = list(thai_stopwords())\r\n",
        "    sentence = word_tokenize(text)\r\n",
        "    result = [word for word in sentence if word not in stop_word and \" \" not in word]\r\n",
        "    return \" /\".join(result)\r\n",
        "\r\n",
        "def tokenize(d):  \r\n",
        "    result = d.split(\"/\")\r\n",
        "    result = list(filter(None, result))\r\n",
        "    return result\r\n",
        "\r\n",
        "new_text = []\r\n",
        "for txt in df[\"text\"]:\r\n",
        "    new_text.append(cleanText(txt))\r\n",
        "\r\n",
        "\r\n",
        "vectorizer = CountVectorizer(tokenizer=tokenize)\r\n",
        "transformed_data = vectorizer.fit_transform(new_text)\r\n",
        "keyword_df1 = pd.DataFrame(columns = ['word', 'count'])\r\n",
        "keyword_df1['word'] = vectorizer.get_feature_names()\r\n",
        "keyword_df1['count'] = np.ravel(transformed_data.sum(axis=0))   \r\n",
        "keyword_df1.sort_values(by=['count'], ascending=False).head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TH4Hj9KixMl8"
      },
      "source": [
        "df.groupby('date').sum()[['retweet_count']].plot(kind=\"bar\",figsize=(12, 6))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w1H1vCgSxc6c"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}